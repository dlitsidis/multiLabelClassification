{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10552170,"sourceType":"datasetVersion","datasetId":6528963}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["# Install Hugging Face Transformers (if not installed)\n","!pip install transformers\n","\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from transformers import DistilBertTokenizer, TFDistilBertModel\n","from transformers import BertTokenizer, TFBertModel\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","# 1️⃣ Load Dataset\n","file_path = \"/kaggle/input/imbd-dataset/IMBD.csv\"\n","df = pd.read_csv(file_path)\n","\n","# 2️⃣ Preprocess Dataset - Keep relevant columns\n","df = df[['description', 'genre']].dropna()\n","\n","# Convert genre string into a list (e.g., \"Drama, Action\" → [\"Drama\", \"Action\"])\n","df['genre'] = df['genre'].apply(lambda x: [g.strip() for g in x.split(',')])\n","\n","# Count occurrences of each genre\n","genre_counts = Counter([g for genres in df['genre'] for g in genres])\n","\n","# Define threshold (e.g., keep genres appearing in at least 100 movies)\n","min_genre_count = 500\n","valid_genres = {g for g, count in genre_counts.items() if count >= min_genre_count}\n","\n","# Filter dataset\n","# Remove \"Animation\" explicitly\n","df['genre'] = df['genre'].apply(lambda genres: [g for g in genres if g in valid_genres and g != \"Animation\"])\n","\n","# Remove empty genre rows\n","df = df[df['genre'].map(len) > 0].reset_index(drop=True)\n","\n","# 3️⃣ Multi-Label Encoding of Genres\n","mlb = MultiLabelBinarizer()\n","y = mlb.fit_transform(df['genre'])  # Convert genres into binary labels\n","\n","# Display available genre labels\n","print(\"Genres:\", mlb.classes_)\n","\n","# 4️⃣ Check Genre Imbalance\n","genre_counts = Counter([g for genres in df['genre'] for g in genres])\n","plt.figure(figsize=(12, 6))\n","plt.bar(genre_counts.keys(), genre_counts.values(), color='skyblue')\n","plt.xticks(rotation=90)\n","plt.xlabel(\"Genres\")\n","plt.ylabel(\"Count\")\n","plt.title(\"Genre Distribution in Dataset\")\n","plt.show()\n","\n","# 5️⃣ Tokenize Movie Descriptions using DistilBERT\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","max_length = 512  # Set max length for input sequences\n","\n","tokens = tokenizer(\n","    list(df['description']),\n","    max_length=max_length,\n","    padding=\"max_length\",\n","    truncation=True,\n","    return_tensors=\"np\"\n",")\n","\n","# 6️⃣ Prepare Train/Test Split\n","X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(\n","    tokens[\"input_ids\"], tokens[\"attention_mask\"], y, test_size=0.2, random_state=42\n",")\n","\n","# Now, split train into train and validation\n","X_train_ids, X_val_ids, X_train_mask, X_val_mask, y_train, y_val = train_test_split(\n","    X_train_ids, X_train_mask, y_train, test_size=0.2, random_state=42\n",")\n","\n","# Convert to dictionary format\n","X_train = {\"input_ids\": X_train_ids, \"attention_mask\": X_train_mask}\n","X_val = {\"input_ids\": X_val_ids, \"attention_mask\": X_val_mask}\n","X_test = {\"input_ids\": X_test_ids, \"attention_mask\": X_test_mask}\n","\n","# Convert to TensorFlow Dataset\n","batch_size = 16\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n","val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n","test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n","\n","\n","from transformers import TFDistilBertModel, DistilBertTokenizer\n","import tensorflow as tf\n","\n","def build_model():\n","    max_length = 512\n","    distilbert_model_name = \"distilbert-base-uncased\"\n","    distilbert_model = TFDistilBertModel.from_pretrained(distilbert_model_name)\n","\n","    # Inputs\n","    input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n","    attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n","\n","    # DistilBERT model\n","    distilbert_output = distilbert_model([input_ids, attention_mask])\n","\n","    # Extract the hidden state of the [CLS] token (first token) from the output\n","    cls_token = distilbert_output.last_hidden_state[:, 0, :]  # (None, 768)\n","\n","\n","    # LSTM layer to capture the sequential context\n","   # lstm_output = tf.keras.layers.LSTM(128, return_sequences=True)(distilbert_output.last_hidden_state)\n","    lstm_output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(distilbert_output.last_hidden_state)\n","\n","    # Combine both the LSTM output and CLS token\n","    combined_output = tf.keras.layers.concatenate([cls_token, lstm_output])\n","\n","    # Additional layers\n","    x = tf.keras.layers.Dense(512, activation=\"relu\")(combined_output)\n","    x = tf.keras.layers.Dropout(0.4)(x)\n","    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n","    x = tf.keras.layers.Dropout(0.3)(x)\n","\n","\n","    # Output layer (binary classification in this case)\n","    output = tf.keras.layers.Dense(y.shape[1], activation=\"sigmoid\")(x)\n","\n","    # Model definition\n","    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n","\n","    # Compile the model\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n","        loss=\"binary_crossentropy\",\n","        metrics=[\"accuracy\"]\n","    )\n","\n","    return model\n","\n","model = build_model()\n","model.summary()\n","\n","# 8️⃣ Train Model\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n","\n","history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=[early_stopping])\n","\n","# 9️⃣ Evaluate Model\n","loss, accuracy = model.evaluate(test_dataset)\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","\n","# 1️⃣1️⃣ Function for Predicting Genres from New Descriptions\n","def predict_genres(description, threshold=0.3):  # Adjust threshold to balance precision/recall\n","    tokens = tokenizer(description, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n","    prediction = model.predict({\"input_ids\": tokens[\"input_ids\"], \"attention_mask\": tokens[\"attention_mask\"]})[0]\n","\n","    predicted_labels = [mlb.classes_[i] for i, prob in enumerate(prediction) if prob > threshold]\n","    return predicted_labels\n","\n","# Example Prediction\n","example_desc = \"A detective must solve a mystery in a futuristic city filled with crime and corruption.\"\n","print(\"Predicted Genres:\", predict_genres(example_desc))"],"metadata":{"trusted":true,"id":"Y7PKkS-9fFCu"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n","import numpy as np\n","\n","def calculate_metrics(y_true, y_pred, threshold=0.5):\n","    \"\"\"\n","    Calculate accuracy, precision, recall, and F1-score for multi-label classification.\n","\n","    Args:\n","        y_true: Ground truth labels (binary multi-label matrix).\n","        y_pred: Predicted probabilities (binary multi-label matrix).\n","        threshold: Probability threshold to classify predictions as 1.\n","\n","    Returns:\n","        A dictionary with accuracy, precision, recall, and F1-score.\n","    \"\"\"\n","    # Binarize predictions based on the threshold\n","    y_pred_binarized = (y_pred > threshold).astype(int)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_true, y_pred_binarized)\n","    precision = precision_score(y_true, y_pred_binarized, average=\"micro\")  # Micro average for multi-label\n","    recall = recall_score(y_true, y_pred_binarized, average=\"micro\")\n","    f1 = f1_score(y_true, y_pred_binarized, average=\"micro\")\n","    f1_samples = f1_score(y_true, y_pred_binarized, average=\"samples\")\n","    hamming = hamming_loss(y_true,y_pred_binarized)\n","\n","    return {\n","        \"accuracy\": accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1_score\": f1,\n","        \"f1_samples\": f1_samples,\n","        \"hamming\": hamming\n","    }\n","\n","# Evaluate on test data\n","y_test_pred = model.predict({\"input_ids\": X_test[\"input_ids\"], \"attention_mask\": X_test[\"attention_mask\"]})\n","metrics = calculate_metrics(y_test, y_test_pred)\n","print(\"Evaluation Metrics on Test Set:\")\n","print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n","print(f\"Precision: {metrics['precision']:.4f}\")\n","print(f\"Recall: {metrics['recall']:.4f}\")\n","print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n","print(f\"F1-Score-Samples: {metrics['f1_samples']:.4f}\")\n","print(f\"Hamming Loss: {metrics['hamming']:.4f}\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T11:57:33.961308Z","iopub.execute_input":"2025-01-23T11:57:33.961618Z","iopub.status.idle":"2025-01-23T12:01:42.525497Z","shell.execute_reply.started":"2025-01-23T11:57:33.961595Z","shell.execute_reply":"2025-01-23T12:01:42.524508Z"},"id":"elHuL0EBfFCz","outputId":"2af0515c-e584-4563-f052-76dd3b073314"},"outputs":[{"name":"stdout","text":"735/735 [==============================] - 247s 337ms/step\nEvaluation Metrics on Test Set:\nAccuracy: 0.5804\nPrecision: 0.6643\nRecall: 0.6107\nF1-Score: 0.6364\nF1-Score-Samples: 0.6190\nHamming Loss: 0.0265\n","output_type":"stream"}],"execution_count":null}]}